# Spider2-Lite Evaluation with Couchbase IQ - Complete Setup

## ğŸ“‹ Summary

I've created a complete evaluation framework for testing **Couchbase IQ** against the **Spider2-Lite** text-to-SQL benchmark. Everything is ready to use!

## ğŸ What's Been Created

### ğŸ“„ Documentation Files

| File | Purpose | Start Here? |
|------|---------|-------------|
| **SETUP.md** | Installation & environment setup | â­ YES - Start here |
| **QUICK_REFERENCE.md** | Quick command cheat sheet | â­ YES - For lookup |
| **COUCHBASE_IQ_GUIDE.md** | Detailed integration guide | Read after setup |
| **TEST_DATA_README.md** | How to use test data | For testing |
| **README_EVALUATION_SETUP.md** | This file - overview | Overview |

### ğŸ”§ Python Scripts

| Script | Purpose | Modify? |
|--------|---------|---------|
| **create_synthetic_test_data.py** | Generates test submissions | No - run as-is |
| **couchbase_iq_integration_template.py** | Template for your integration | â­ YES - Customize this |
| **evaluate.py** | Main evaluation script | No - provided |

### ğŸ“ Test Data Folders (Auto-generated)

| Folder | Content | Expected Score |
|--------|---------|----------------|
| `test_submission_csv_perfect/` | Perfect CSV matches | 100% âœ… |
| `test_submission_csv_wrong/` | Wrong CSV data | 0% âŒ |
| `test_submission_sql_perfect/` | Perfect SQL matches | 100% âœ… |
| `test_submission_sql_mixed/` | Mixed correct/wrong | ~50% âš–ï¸ |

### ğŸ“‚ Your Output Folder

| Folder | Purpose | You Create |
|--------|---------|------------|
| `couchbase_iq_results/` | Your generated SQL/CSV files | â­ YES |

## ğŸš€ Quick Start (5 Minutes)

### 1. Install Dependencies (1 min)

```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate

# Install packages
pip install pandas tqdm
```

### 2. Generate Test Data (1 min)

```bash
python3 create_synthetic_test_data.py
```

**Output**: Creates 4 test folders with synthetic data

### 3. Test Evaluation - No Database Needed! (1 min)

```bash
# Test with perfect data (should get 100%)
python3 evaluate.py --result_dir test_submission_csv_perfect --mode exec_result
```

**Expected Output**:
```
Final score: 1.0, Correct examples: 4, Total examples: 4
```

### 4. Customize Integration Template (2 min)

Edit `couchbase_iq_integration_template.py`:

```python
def generate_sql_with_couchbase_iq(question, database, schema, external_docs):
    # Replace stub with your Couchbase IQ call
    # Example:
    from your_couchbase_iq_sdk import Client
    client = Client(api_key="your-key")
    return client.generate_sql(
        question=question,
        schema=schema,
        context=external_docs
    )
```

### 5. Run Your Evaluation

```bash
# Generate SQL files
python3 couchbase_iq_integration_template.py

# Evaluate (CSV mode - no DB required)
python3 evaluate.py --result_dir couchbase_iq_results --mode exec_result

# Or SQL mode (requires DB access)
python3 evaluate.py --result_dir couchbase_iq_results --mode sql
```

## ğŸ“Š Understanding the Evaluation

### Input: Spider2-Lite Questions (547 total)

Located in: `../spider2-lite.jsonl`

```json
{
  "instance_id": "bq001",
  "db": "ga360", 
  "question": "For each visitor who made at least one transaction...",
  "external_knowledge": "google_analytics_sample.ga_sessions.md"
}
```

### Your Task: Generate SQL

For each question, generate SQL and save as `{instance_id}.sql`:

```
couchbase_iq_results/
â”œâ”€â”€ bq001.sql
â”œâ”€â”€ bq002.sql
â”œâ”€â”€ local003.sql
â””â”€â”€ ...
```

### Evaluation: Compare Results

The evaluator:
1. Executes your SQL (or reads your CSVs)
2. Compares with gold standard results
3. Returns accuracy score

## ğŸ¯ Two Evaluation Modes

### Mode 1: SQL Execution (Automatic)

```bash
python3 evaluate.py --result_dir couchbase_iq_results --mode sql
```

- **Input**: `.sql` files
- **Process**: Executes on real databases
- **Output**: Accuracy score
- **Requires**: Database access (BigQuery costs $$$)

### Mode 2: CSV Results (Manual)

```bash
python3 evaluate.py --result_dir couchbase_iq_results --mode exec_result
```

- **Input**: `.csv` files (pre-executed results)
- **Process**: Direct comparison
- **Output**: Accuracy score
- **Requires**: Nothing (free!)

**Recommendation**: Use CSV mode during development to avoid costs.

## ğŸ“ Complete Folder Structure

```
evaluation_suite/
â”‚
â”œâ”€â”€ ğŸ“˜ DOCUMENTATION
â”‚   â”œâ”€â”€ SETUP.md                          # â­ Start here: Install & setup
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md                # â­ Command cheat sheet
â”‚   â”œâ”€â”€ COUCHBASE_IQ_GUIDE.md            # Detailed integration guide
â”‚   â”œâ”€â”€ TEST_DATA_README.md               # Test data usage
â”‚   â””â”€â”€ README_EVALUATION_SETUP.md        # This file
â”‚
â”œâ”€â”€ ğŸ”§ SCRIPTS
â”‚   â”œâ”€â”€ create_synthetic_test_data.py     # Generates test data
â”‚   â”œâ”€â”€ couchbase_iq_integration_template.py  # â­ Customize this
â”‚   â””â”€â”€ evaluate.py                       # Evaluation script (provided)
â”‚
â”œâ”€â”€ ğŸ§ª TEST DATA (auto-generated)
â”‚   â”œâ”€â”€ test_submission_csv_perfect/      # 100% accuracy test
â”‚   â”œâ”€â”€ test_submission_csv_wrong/        # 0% accuracy test
â”‚   â”œâ”€â”€ test_submission_sql_perfect/      # 100% accuracy test
â”‚   â””â”€â”€ test_submission_sql_mixed/        # ~50% accuracy test
â”‚
â”œâ”€â”€ ğŸ¯ YOUR OUTPUT
â”‚   â””â”€â”€ couchbase_iq_results/             # â­ Create this folder
â”‚       â”œâ”€â”€ bq001.sql                     # Your generated SQL
â”‚       â”œâ”€â”€ bq002.sql
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“Š REFERENCE DATA (provided)
â”‚   â””â”€â”€ gold/
â”‚       â”œâ”€â”€ sql/                          # Gold SQL queries
â”‚       â”œâ”€â”€ exec_result/                  # Gold results
â”‚       â””â”€â”€ spider2lite_eval.jsonl        # Eval config
â”‚
â””â”€â”€ ğŸ”‘ CREDENTIALS (you provide)
    â”œâ”€â”€ bigquery_credential.json          # For BigQuery
    â””â”€â”€ snowflake_credential.json         # For Snowflake
```

## ğŸ“ Evaluation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Load Questions                                           â”‚
â”‚    spider2-lite.jsonl (547 questions)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Generate SQL with Couchbase IQ                           â”‚
â”‚    â€¢ Load database schemas                                   â”‚
â”‚    â€¢ Load external knowledge                                â”‚
â”‚    â€¢ Call Couchbase IQ API                                  â”‚
â”‚    â€¢ Save {instance_id}.sql                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Evaluate                                                 â”‚
â”‚    â€¢ Execute SQL on databases (or load CSVs)                â”‚
â”‚    â€¢ Compare with gold results                              â”‚
â”‚    â€¢ Calculate accuracy                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Results                                                  â”‚
â”‚    â€¢ Accuracy score                                         â”‚
â”‚    â€¢ Per-instance results                                   â”‚
â”‚    â€¢ Error analysis                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Instance Distribution

| Prefix | Database | Count | Cost | Notes |
|--------|----------|-------|------|-------|
| `bq*` | BigQuery | ~350 | $$$ | Google Cloud |
| `ga*` | BigQuery | ~50 | $$$ | Analytics |
| `local*` | SQLite | ~100 | Free | Local files |
| `sf_*` | Snowflake | ~47 | $ | Test account |
| **Total** | - | **547** | - | Full benchmark |

## ğŸ’¡ Best Practices

### For Development
1. âœ… Use CSV mode (free, fast)
2. âœ… Test on 5-10 instances first
3. âœ… Use synthetic test data
4. âœ… Focus on `local*` instances (SQLite, free)

### For Production
1. âœ… Set up all databases
2. âœ… Use SQL mode (full automation)
3. âœ… Run on all 547 instances
4. âœ… Budget for BigQuery costs (~$5-50)

## ğŸ“ˆ Expected Performance

Based on Spider2 paper benchmarks:

| Method | Accuracy | Notes |
|--------|----------|-------|
| GPT-4 + RAG | 35-45% | State-of-the-art |
| Fine-tuned T5 | 25-35% | Specialized models |
| Rule-based | 15-20% | Baseline |
| **Your Goal** | **?%** | Test Couchbase IQ! |

## ğŸ¯ Your Next Steps

1. **Read** `SETUP.md` - Install dependencies
2. **Run** `python3 create_synthetic_test_data.py`
3. **Test** with CSV mode (no DB needed)
4. **Edit** `couchbase_iq_integration_template.py`
5. **Generate** SQL with Couchbase IQ
6. **Evaluate** your results
7. **Analyze** performance

## ğŸ“ Questions?

- **Setup issues?** â†’ Read `SETUP.md`
- **Quick commands?** â†’ Read `QUICK_REFERENCE.md`
- **Integration details?** â†’ Read `COUCHBASE_IQ_GUIDE.md`
- **Testing?** â†’ Read `TEST_DATA_README.md`

## âœ… Validation Checklist

Before full evaluation:

- [ ] Python 3.7+ installed
- [ ] Dependencies installed (pandas, tqdm)
- [ ] Test data generated (4 folders)
- [ ] Test evaluation runs (100% on perfect data)
- [ ] Template script runs (creates placeholder SQLs)
- [ ] Couchbase IQ integration customized
- [ ] Database credentials configured (if using SQL mode)
- [ ] Output folder created (`couchbase_iq_results/`)

## ğŸ‰ You're Ready!

Everything is set up. Just follow `SETUP.md` to install dependencies and you can start evaluating Couchbase IQ against Spider2-Lite!

---

**Created**: October 31, 2025
**Location**: `/Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite/`
**Purpose**: Evaluate Couchbase IQ's text-to-SQL capabilities on the Spider2-Lite benchmark

