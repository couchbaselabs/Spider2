# Synthetic Test Data for Spider2-Lite Evaluation

This folder contains synthetic test submissions to validate the evaluation framework.

## Test Folders

### 1. test_submission_sql_perfect/
- **Mode**: SQL
- **Expected Score**: 100%
- **Description**: Contains exact copies of gold SQL queries
- **Run**: `python evaluate.py --result_dir test_submission_sql_perfect --mode sql`

### 2. test_submission_sql_mixed/
- **Mode**: SQL
- **Expected Score**: ~50%
- **Description**: Mix of correct queries and intentionally wrong ones
- **Run**: `python evaluate.py --result_dir test_submission_sql_mixed --mode sql`

### 3. test_submission_csv_perfect/
- **Mode**: exec_result (CSV)
- **Expected Score**: 100%
- **Description**: Contains exact copies of gold execution results
- **Run**: `python evaluate.py --result_dir test_submission_csv_perfect --mode exec_result`

### 4. test_submission_csv_wrong/
- **Mode**: exec_result (CSV)
- **Expected Score**: 0%
- **Description**: Contains empty results (wrong answers)
- **Run**: `python evaluate.py --result_dir test_submission_csv_wrong --mode exec_result`

## Quick Test

Run all tests:
```bash
# Test perfect SQL submission
python evaluate.py --result_dir test_submission_sql_perfect --mode sql

# Test mixed SQL submission  
python evaluate.py --result_dir test_submission_sql_mixed --mode sql

# Test perfect CSV submission
python evaluate.py --result_dir test_submission_csv_perfect --mode exec_result

# Test wrong CSV submission
python evaluate.py --result_dir test_submission_csv_wrong --mode exec_result
```

## For Couchbase IQ Evaluation

1. Create a new folder: `couchbase_iq_results/`
2. Generate SQL files for each instance_id: `{instance_id}.sql`
3. Run: `python evaluate.py --result_dir couchbase_iq_results --mode sql`

## Instance ID Prefixes
- `bq*` = BigQuery queries
- `ga*` = Google Analytics (BigQuery)
- `local*` = SQLite (local database)
- `sf_*` = Snowflake queries

Generated by: create_synthetic_test_data.py
