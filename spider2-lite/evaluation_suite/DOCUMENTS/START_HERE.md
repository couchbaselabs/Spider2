# ğŸ¯ START HERE: Couchbase IQ + Spider2-Lite Evaluation

## âœ¨ What's Been Set Up For You

I've created a **complete evaluation framework** to test Couchbase IQ against the Spider2-Lite benchmark. Everything you need is ready!

## ğŸ“š Documentation (Read in This Order)

### 1ï¸âƒ£ **SETUP.md** â­ START HERE
- Install Python dependencies
- Set up databases (optional)
- Verify installation
- **Time**: 5 minutes

### 2ï¸âƒ£ **QUICK_REFERENCE.md** â­ KEEP OPEN
- Command cheat sheet
- Quick lookup table
- Common issues
- **Use**: While working

### 3ï¸âƒ£ **COUCHBASE_IQ_GUIDE.md**
- Detailed integration guide
- Database schemas
- Best practices
- **Read**: Before customization

### 4ï¸âƒ£ **TEST_DATA_README.md**
- How to use test data
- Expected results
- **Use**: For testing

### 5ï¸âƒ£ **README_EVALUATION_SETUP.md**
- Complete overview
- Folder structure
- Evaluation flow
- **Reference**: Full details

## ğŸš€ Quick Start (Copy & Paste)

```bash
# 1. Navigate to evaluation suite
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite

# 2. Set up virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install pandas tqdm

# 4. Generate test data
python3 create_synthetic_test_data.py

# 5. Test evaluation (no database needed!)
python3 evaluate.py --result_dir test_submission_csv_perfect --mode exec_result

# Expected: "Final score: 1.0" âœ…
```

## ğŸ¨ Visual Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPIDER2-LITE: 547 Text-to-SQL Test Questions               â”‚
â”‚  Input: Natural language question                            â”‚
â”‚  Output: SQL query                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOUR COUCHBASE IQ INTEGRATION                               â”‚
â”‚  â€¢ Load question + database schema                           â”‚
â”‚  â€¢ Call Couchbase IQ API                                     â”‚
â”‚  â€¢ Generate SQL                                              â”‚
â”‚  â€¢ Save to: couchbase_iq_results/{instance_id}.sql          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EVALUATION                                                  â”‚
â”‚  â€¢ Execute SQL on databases (or use pre-executed CSVs)       â”‚
â”‚  â€¢ Compare with gold standard results                        â”‚
â”‚  â€¢ Calculate accuracy score                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESULTS                                                     â”‚
â”‚  â€¢ Accuracy: X / 547 (Y%)                                    â”‚
â”‚  â€¢ Per-instance breakdown                                    â”‚
â”‚  â€¢ Error analysis                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ What You Have Now

```
evaluation_suite/
â”‚
â”œâ”€â”€ ğŸ“˜ DOCS (5 comprehensive guides)
â”‚   â”œâ”€â”€ START_HERE.md                 â­ THIS FILE
â”‚   â”œâ”€â”€ SETUP.md                      â­ Read first
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md            â­ Keep open
â”‚   â”œâ”€â”€ COUCHBASE_IQ_GUIDE.md        
â”‚   â”œâ”€â”€ TEST_DATA_README.md
â”‚   â””â”€â”€ README_EVALUATION_SETUP.md
â”‚
â”œâ”€â”€ ğŸ”§ SCRIPTS (ready to use)
â”‚   â”œâ”€â”€ create_synthetic_test_data.py        # Run to generate tests
â”‚   â”œâ”€â”€ couchbase_iq_integration_template.py # â­ Customize this
â”‚   â””â”€â”€ evaluate.py                           # Evaluation engine
â”‚
â”œâ”€â”€ ğŸ§ª TEST DATA (4 test folders - auto-generated)
â”‚   â”œâ”€â”€ test_submission_csv_perfect/   # 100% accuracy
â”‚   â”œâ”€â”€ test_submission_csv_wrong/     # 0% accuracy
â”‚   â”œâ”€â”€ test_submission_sql_perfect/   # 100% accuracy
â”‚   â””â”€â”€ test_submission_sql_mixed/     # ~50% accuracy
â”‚
â””â”€â”€ ğŸ¯ YOUR WORK
    â””â”€â”€ couchbase_iq_results/          # Create this, put your SQLs here
```

## ğŸ¯ Your Mission

### Goal
Evaluate Couchbase IQ's text-to-SQL performance on Spider2-Lite benchmark

### Input
547 natural language questions â†’ SQL queries

### Output
Accuracy score (% of correct SQL queries)

### Baseline
- State-of-the-art (GPT-4): ~35-45%
- Your goal: Beat this! ğŸ†

## ğŸ”¥ Three Ways to Evaluate

### Option 1: CSV Mode (FREE, Easy) â­ Recommended for Dev
```bash
# You provide: {instance_id}.csv files (pre-executed results)
# Evaluator: Compares CSVs with gold results
python3 evaluate.py --result_dir couchbase_iq_results --mode exec_result
```
**Pros**: No database setup, free, fast
**Cons**: You must execute SQL separately

### Option 2: SQL Mode - SQLite Only (FREE)
```bash
# You provide: {instance_id}.sql files
# Evaluator: Executes on SQLite databases (~100 instances)
python3 evaluate.py --result_dir couchbase_iq_results --mode sql
```
**Pros**: Automatic execution, free
**Cons**: Only ~100 instances (not full benchmark)

### Option 3: SQL Mode - Full (Costs $$$)
```bash
# You provide: {instance_id}.sql files  
# Evaluator: Executes on ALL databases (547 instances)
python3 evaluate.py --result_dir couchbase_iq_results --mode sql
```
**Pros**: Full benchmark, automatic
**Cons**: BigQuery costs ~$5-50

## âš¡ One-Minute Test

```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite

# Generate test data
python3 create_synthetic_test_data.py

# Run test (no DB needed!)
python3 evaluate.py --result_dir test_submission_csv_perfect --mode exec_result

# Should output: "Final score: 1.0" âœ…
```

If this works, you're ready! ğŸ‰

## ğŸ“ Customization Steps

### 1. Edit Integration Template

Open: `couchbase_iq_integration_template.py`

Find the function:
```python
def generate_sql_with_couchbase_iq(question, database, schema, external_docs):
    # TODO: Replace this stub with your Couchbase IQ API call
    ...
```

Replace with:
```python
def generate_sql_with_couchbase_iq(question, database, schema, external_docs):
    # Your Couchbase IQ integration
    from couchbase_iq import Client  # Your SDK
    client = Client(api_key="your-key")
    
    result = client.generate_sql(
        question=question,
        schema=schema,
        context=external_docs
    )
    
    return result.sql
```

### 2. Run Integration Script

```bash
python3 couchbase_iq_integration_template.py
```

This generates: `couchbase_iq_results/{instance_id}.sql` for each question

### 3. Evaluate

```bash
# CSV mode (free)
python3 evaluate.py --result_dir couchbase_iq_results --mode exec_result

# SQL mode (requires DB)
python3 evaluate.py --result_dir couchbase_iq_results --mode sql
```

## ğŸ“ Learning Path

1. **Understand** the benchmark (10 min)
   - Read: `SETUP.md` 
   - Explore: `../spider2-lite.jsonl`

2. **Test** the framework (5 min)
   - Generate test data
   - Run test evaluation
   - Verify 100% accuracy

3. **Integrate** Couchbase IQ (30 min)
   - Edit template script
   - Test on 5 instances
   - Debug integration

4. **Evaluate** (1-2 hours)
   - Run on all 547 instances
   - Analyze results
   - Iterate and improve

## ğŸ†˜ Need Help?

### Quick Questions
â†’ Read `QUICK_REFERENCE.md` (command cheat sheet)

### Setup Issues
â†’ Read `SETUP.md` (installation guide)

### Integration Details
â†’ Read `COUCHBASE_IQ_GUIDE.md` (detailed guide)

### Test Data
â†’ Read `TEST_DATA_README.md` (testing guide)

### Full Overview
â†’ Read `README_EVALUATION_SETUP.md` (complete details)

## âœ… Pre-Flight Checklist

Before full evaluation:

- [ ] Read `SETUP.md`
- [ ] Python 3.7+ installed
- [ ] Dependencies installed: `pip install pandas tqdm`
- [ ] Test data generated: `python3 create_synthetic_test_data.py`
- [ ] Test evaluation passed: 100% on `test_submission_csv_perfect`
- [ ] Integration template customized
- [ ] Tested on 5-10 instances
- [ ] Ready for full evaluation!

## ğŸ‰ You're All Set!

Everything is prepared. Just:

1. âœ… Follow `SETUP.md` to install dependencies (5 min)
2. âœ… Run test evaluation (1 min)
3. âœ… Customize integration template (30 min)
4. âœ… Evaluate Couchbase IQ (1-2 hours)

**Let's see how Couchbase IQ performs! ğŸš€**

---

**Questions?** All documentation is in this folder.
**Ready?** Start with `SETUP.md`!

