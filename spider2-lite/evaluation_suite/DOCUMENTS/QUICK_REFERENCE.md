# Spider2-Lite + Couchbase IQ: Quick Reference Card

## ğŸ“ Folder Structure You Need

```
evaluation_suite/
â”œâ”€â”€ couchbase_iq_results/          # CREATE THIS - Your SQL outputs
â”‚   â”œâ”€â”€ bq001.sql
â”‚   â”œâ”€â”€ bq002.sql
â”‚   â””â”€â”€ ...
â”œâ”€â”€ gold/                           # PROVIDED - Reference answers
â”‚   â”œâ”€â”€ sql/                       # Gold SQL queries
â”‚   â””â”€â”€ exec_result/               # Gold execution results
â”œâ”€â”€ evaluate.py                     # PROVIDED - Evaluation script
â””â”€â”€ couchbase_iq_integration_template.py  # TEMPLATE - Customize this
```

## ğŸš€ Quick Start (3 Steps)

### 1ï¸âƒ£ Generate SQL files

```python
# Customize couchbase_iq_integration_template.py
# Replace the stub with your Couchbase IQ API call
python3 couchbase_iq_integration_template.py
```

### 2ï¸âƒ£ Run evaluation

```bash
# SQL mode (requires DB access)
python3 evaluate.py --result_dir couchbase_iq_results --mode sql

# CSV mode (no DB needed - if you pre-executed)
python3 evaluate.py --result_dir couchbase_iq_results --mode exec_result
```

### 3ï¸âƒ£ Check results

Results in: `log.txt` and console output

## ğŸ“Š Two Evaluation Modes

| Mode | Input | Pros | Cons | Command |
|------|-------|------|------|---------|
| **SQL** | `.sql` files | Tests actual SQL execution | Requires DB access, costs $ | `--mode sql` |
| **CSV** | `.csv` files | No DB needed, free | Must execute SQL separately | `--mode exec_result` |

## ğŸ“ Input Data Format

Questions in `../spider2-lite.jsonl`:

```json
{
  "instance_id": "bq001",           // File name without extension
  "db": "ga360",                    // Database identifier
  "question": "Natural language...", // What to convert to SQL
  "external_knowledge": "doc.md"    // Optional context file
}
```

## ğŸ“¤ Output Format

### SQL Mode Output:
```
couchbase_iq_results/
â”œâ”€â”€ bq001.sql      # Your generated SQL
â”œâ”€â”€ bq002.sql
â””â”€â”€ ...
```

### CSV Mode Output:
```
couchbase_iq_results/
â”œâ”€â”€ bq001.csv      # Executed results
â”œâ”€â”€ bq002.csv
â””â”€â”€ ...
```

## ğŸ¯ Instance ID Prefixes

| Prefix | Database | Example | Notes |
|--------|----------|---------|-------|
| `bq*` | BigQuery | `bq001.sql` | Google Cloud, costs $ |
| `ga*` | BigQuery (GA) | `ga010.sql` | Analytics data |
| `local*` | SQLite | `local003.sql` | Free, local files |
| `sf_*` | Snowflake | `sf_bq026.sql` | Requires account |

## ğŸ§ª Test Before Real Eval

```bash
# Test with synthetic data (no DB needed!)
cd evaluation_suite

# Should get 100% accuracy
python3 evaluate.py --result_dir test_submission_csv_perfect --mode exec_result

# Should get 0% accuracy
python3 evaluate.py --result_dir test_submission_csv_wrong --mode exec_result
```

## ğŸ”§ Couchbase IQ Integration Points

Edit `couchbase_iq_integration_template.py`:

```python
def generate_sql_with_couchbase_iq(question, database, schema, external_docs):
    # Replace this stub:
    
    # Option 1: REST API
    response = requests.post("https://api.couchbase-iq.com/generate", 
                            json={"question": question, "schema": schema})
    return response.json()["sql"]
    
    # Option 2: SDK
    from couchbase_iq import Client
    client = Client(api_key="...")
    return client.generate_sql(question, schema)
```

## ğŸ“ˆ What Gets Evaluated

âœ… Column names match
âœ… Row values match (Â±0.01 for floats)
âœ… Row order (if required)
âœ… All expected columns present

## ğŸ—‚ï¸ Helper Files Created

| File | Purpose |
|------|---------|
| `COUCHBASE_IQ_GUIDE.md` | Comprehensive integration guide |
| `QUICK_REFERENCE.md` | This file - quick lookup |
| `TEST_DATA_README.md` | How to test with synthetic data |
| `create_synthetic_test_data.py` | Generates test submissions |
| `couchbase_iq_integration_template.py` | Template to customize |

## ğŸ’¡ Pro Tips

1. **Start small**: Test on 5-10 questions first
2. **Use CSV mode for dev**: Avoid BigQuery costs
3. **Check external_knowledge**: Some questions need extra docs
4. **Load schemas**: Schema files in `../resource/databases/`
5. **Test syntax**: Each DB has different SQL dialect

## ğŸ› Common Issues

| Error | Solution |
|-------|----------|
| `No module named 'pandas'` | `pip install pandas` |
| `GOOGLE_APPLICATION_CREDENTIALS` | Set in `bigquery_credential.json` |
| `Error occurred while fetching data` | Check SQL syntax for target DB |
| `No data found` | Query returned empty - check logic |

## ğŸ“ Need Help?

1. Read `COUCHBASE_IQ_GUIDE.md` for detailed walkthrough
2. Check `TEST_DATA_README.md` for testing instructions
3. Run synthetic tests first: `python3 create_synthetic_test_data.py`
4. Review example submissions in `example_submission_folder/`

## ğŸ“ Expected Performance

- **State-of-the-art**: ~35-45% (GPT-4 + RAG)
- **Fine-tuned models**: ~25-35%
- **Rule-based**: ~15-20%

## âš¡ Quick Commands Cheat Sheet

```bash
# Generate test data
python3 create_synthetic_test_data.py

# Test evaluation (CSV mode - no DB)
python3 evaluate.py --result_dir test_submission_csv_perfect --mode exec_result

# Run your Couchbase IQ integration
python3 couchbase_iq_integration_template.py

# Evaluate Couchbase IQ results (SQL mode)
python3 evaluate.py --result_dir couchbase_iq_results --mode sql

# View results
cat log.txt
```

---

**Ready? Start here:**
1. âœ… Run `python3 create_synthetic_test_data.py`
2. âœ… Test with `python3 evaluate.py --result_dir test_submission_csv_perfect --mode exec_result`
3. âœ… Customize `couchbase_iq_integration_template.py`
4. âœ… Generate SQLs and evaluate!

