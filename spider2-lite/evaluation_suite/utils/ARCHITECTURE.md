# Spider2-Lite Evaluation Suite: Complete Architecture

## Table of Contents

1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Part I: Data Migration Pipeline](#part-i-data-migration-pipeline)
4. [Part II: Testing & SQL Generation](#part-ii-testing--sql-generation)
5. [Part III: Evaluation & Scoring](#part-iii-evaluation--scoring)
6. [Part IV: Utility Tools](#part-iv-utility-tools)
7. [Complete Workflow Guide](#complete-workflow-guide)
8. [Quick Reference](#quick-reference)

---

## Overview

### What is Spider2-Lite?

Spider2-Lite is a text-to-SQL benchmark that evaluates the ability of systems to generate SQL queries from natural language questions. This evaluation suite specifically tests **Couchbase IQ**, an AI-powered natural language to SQL++ translation service.

### System Components

The complete evaluation system consists of four main parts:

1. **Data Migration** - Convert SQLite databases to Couchbase
2. **Testing** - Generate SQL queries using Couchbase IQ
3. **Evaluation** - Compare generated results against gold standards
4. **Utilities** - Supporting tools for setup and maintenance

### Why This Architecture?

**Challenge**: Spider2-Lite provides SQLite databases, but we need to evaluate Couchbase IQ (SQL++).

**Solution**: 
- Migrate data: SQLite → JSON → Couchbase
- Test with IQ: Natural language → SQL++ queries
- Evaluate: Compare outputs against SQLite-based gold results
- Focus: Only evaluate "local" test cases (SQLite-compatible)

### Directory Structure

```
Spider2/spider2-lite/
├── local_sqlite/                           # Source SQLite databases
│   ├── AdventureWorks.sqlite
│   ├── California_Schools.sqlite
│   └── ... (more databases)
│
├── local_sqlite_json/                      # Intermediate JSON files
│   ├── AdventureWorks.json
│   ├── California_Schools.json
│   └── ... (generated by export script)
│
├── spider2-lite.jsonl                      # Benchmark questions
│
├── resource/
│   └── databases/
│       └── spider2-localdb/                # Original SQLite DBs
│
└── evaluation_suite/
    ├── couchbase_config.json               # Configuration
    ├── test_couchbase_iq.py                # Quick test script
    ├── couchbase_iq_spider2_evaluator.py   # Full evaluator
    ├── evaluate.py                         # Scoring script
    │
    ├── gold/                               # Reference results
    │   ├── exec_result/
    │   │   ├── local002.csv                # SQLite results
    │   │   ├── local003_a.csv
    │   │   ├── local003_b.csv
    │   │   └── ... (150 local files after cleanup)
    │   ├── sql/                            # Reference SQL queries
    │   └── spider2lite_eval.jsonl          # Evaluation metadata
    │
    ├── iq_results/                         # Generated test results
    │   ├── local002.csv
    │   ├── local003.csv
    │   └── ... (generated during testing)
    │
    └── utils/                              # Utility scripts
        ├── export_sqlite_to_json.py
        ├── batch_import_to_couchbase.py
        ├── cleanup_non_local_results.sh
        ├── get_all_keyspaces.py
        ├── find_mixed_type_columns.py      # Analyze JSON files for mixed type columns
        └── ARCHITECTURE.md (this file)
    │
    └── baselines/codes/utils/              # Additional utility scripts
        └── clean_numeric_columns.py         # Clean empty strings in numeric columns
```

---

## System Architecture

### High-Level Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           SPIDER2-LITE EVALUATION SYSTEM                     │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PHASE 1: DATA MIGRATION                                                     │
│ Purpose: Prepare data for Couchbase IQ testing                             │
└─────────────────────────────────────────────────────────────────────────────┘

    SQLite Databases              JSON Files               Couchbase Server
    (local_sqlite/)               (local_sqlite_json/)     (localhost:8091)
         │                             │                          │
         │                             │                          │
    AdventureWorks.sqlite         AdventureWorks.json       AdventureWorks (bucket)
    California_Schools.sqlite     California_Schools.json     └─ spider2 (scope)
    E_commerce.sqlite             E_commerce.json                ├─ orders
    IMDB.sqlite                   IMDB.json                      ├─ customers
    Baseball.sqlite               Baseball.json                  ├─ products
    ... (more DBs)                ... (more JSON)                └─ ... (collections)
         │                             │                          │
         └──[export_sqlite_to_json]──→ └──[batch_import_to_cb]──→

┌─────────────────────────────────────────────────────────────────────────────┐
│ PHASE 2: TESTING & SQL GENERATION                                           │
│ Purpose: Generate SQL queries from natural language using Couchbase IQ     │
└─────────────────────────────────────────────────────────────────────────────┘

    Natural Language Questions         Couchbase IQ API          SQL++ Queries
    (spider2-lite.jsonl)              (Capella)                 (Generated)
         │                                 │                          │
         │                                 │                          │
    local002: "Calculate 5-day           ┌──────────────┐      SELECT AVG(...)
    moving average..."                   │ Couchbase IQ │      FROM E_commerce...
         │                               │  (NL → SQL)  │           │
    local003: "Calculate RFM      ───→   │              │   ───→    │
    segments..."                         │  API Call    │           │
         │                               └──────────────┘           │
    local004: "Top 3 customers                                      │
    by payment..."                                                  │
         │                                                          │
         └──[test_couchbase_iq.py OR couchbase_iq_spider2_evaluator.py]
                                                                    │
                                                                    ↓
         ┌──────────────────────────────────────────────────────────────┐
         │ Execute SQL++ against Couchbase                              │
         │ (N1QL Query Service - localhost:8093)                        │
         └──────────────────────────────────────────────────────────────┘
                                    │
                                    ↓
                            Test Results (CSV)
                            (iq_results/)
                                 │
                            local002.csv
                            local003.csv
                            local004.csv
                            ... (150 files)

┌─────────────────────────────────────────────────────────────────────────────┐
│ PHASE 3: EVALUATION & SCORING                                               │
│ Purpose: Compare generated results against gold standard                    │
└─────────────────────────────────────────────────────────────────────────────┘

    Gold Results                  Test Results              Evaluation
    (gold/exec_result/)          (iq_results/)             (evaluate.py)
         │                            │                          │
    local002.csv                  local002.csv                  │
    local003_a.csv  ─────┐        local003.csv ───→   Compare & Score
    local003_b.csv  ─────┘        local004.csv          (pandas matching)
    local004_a.csv  ─────┐             │                      │
    local004_b.csv  ─────┘             │                      │
    ... (150 local files)              │                      ↓
         │                             │              ┌─────────────────┐
         └──────[cleanup script]───────┘              │ Final Results:  │
         (removes sf_*, bq_*, ga_*)                   │ Score: 0.847    │
                                                      │ Correct: 127/150│
                                                      └─────────────────┘
```

### Three-Phase Architecture

#### Phase 1: Data Migration
- **Input**: SQLite databases (`.sqlite` files)
- **Process**: Export to JSON → Import to Couchbase
- **Output**: Couchbase buckets with data
- **Scripts**: `export_sqlite_to_json.py`, `batch_import_to_couchbase.py`
- **Frequency**: One-time setup

#### Phase 2: Testing & SQL Generation
- **Input**: Natural language questions (JSONL)
- **Process**: Generate SQL using IQ → Execute → Save results
- **Output**: CSV files with query results
- **Scripts**: `test_couchbase_iq.py`, `couchbase_iq_spider2_evaluator.py`
- **Frequency**: Every evaluation run

#### Phase 3: Evaluation & Scoring
- **Input**: Test results + Gold results
- **Process**: Compare dataframes → Calculate accuracy
- **Output**: Scores and error analysis
- **Scripts**: `cleanup_non_local_results.sh`, `evaluate.py`
- **Frequency**: After each test run

---

## Part I: Data Migration Pipeline

### Overview

The data migration pipeline converts SQLite databases into Couchbase's document model:

```
SQLite Tables → JSON Documents → Couchbase Collections
```

**Why migrate?**
- Couchbase IQ works with Couchbase/Capella data
- Need to test IQ's SQL++ generation capabilities
- Preserve relational structure using scopes/collections

### Step 1: Export SQLite to JSON

#### Script: `export_sqlite_to_json.py`

**Location**: `evaluation_suite/utils/export_sqlite_to_json.py`

**Purpose**: Converts SQLite databases to JSON format for Couchbase import

**Process**:
1. Scans `local_sqlite/` directory for `.sqlite` files
2. For each database:
   - Connects to SQLite
   - Queries all table names
   - Extracts all rows from each table
   - Converts to JSON structure
3. Saves JSON files to `local_sqlite_json/`

**Configuration**:
```python
sqlite_dir = Path("/Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/local_sqlite")
output_dir = Path("/Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/local_sqlite_json")
```

**Usage**:
```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite/utils
python export_sqlite_to_json.py
```

**Output Format** (`{table_name: [rows]}`):

Each JSON file contains a dictionary where:
- **Keys** = table names (become Couchbase collections)
- **Values** = arrays of row objects (become documents)

```json
{
  "orders": [
    {
      "order_id": "00143d0f86d6fbd9f9b38ab440000000",
      "customer_id": "00012a2ce6f8dcda20d059ce98491703",
      "order_status": "delivered",
      "order_purchase_timestamp": "2017-10-02 10:56:33"
    },
    ...
  ],
  "customers": [
    {
      "customer_id": "00012a2ce6f8dcda20d059ce98491703",
      "customer_unique_id": "861eff4711a542e4b93843c6dd7febb0",
      "customer_zip_code_prefix": "14409"
    },
    ...
  ]
}
```

**Import Mapping**:
- JSON file name → Couchbase **bucket** (e.g., `E_commerce.json` → `E_commerce` bucket)
- Each table key → Couchbase **collection** under `spider2` scope
- Each row object → Couchbase **document** in that collection

**Key Functions**:

- `get_all_tables(conn)` - Queries sqlite_master for table names
- `get_table_data(conn, table_name)` - Extracts rows as dictionaries
- `export_sqlite_to_json(sqlite_path, output_path)` - Main export logic
- `main()` - Batch processes all SQLite files

**Example Output**:
```
Found 5 SQLite files to process

============================================================
Processing: AdventureWorks.sqlite
  Found 17 tables
    - salesperson: 17 rows
    - product: 504 rows
    - customer: 847 rows
  ✓ Saved to: AdventureWorks.json
============================================================

✓ All files processed!
Total files created: 5
```

### Step 2: Import JSON to Couchbase

#### Script: `batch_import_to_couchbase.py`

**Location**: `evaluation_suite/utils/batch_import_to_couchbase.py`

**Purpose**: Batch imports JSON files into Couchbase Server

**Process**:
1. Scans `local_sqlite_json/` for `.json` files
2. For each JSON file (e.g., `AdventureWorks.json`):
   - Creates **bucket** (named after JSON file, e.g., `AdventureWorks`)
   - Creates **scope** `spider2` (explicit scope creation before any imports)
   - For each table in JSON (e.g., `salesperson`, `product`):
     - Creates **collection** with table name under `spider2` scope
     - Imports each row as a **document** with metadata (`_table`, `_source`)
3. Reports success/failure for each file

**JSON Structure Handled**:
- Format: `{table_name: [rows]}` - each table maps to an array of row objects
- Each table key becomes a collection, each row becomes a document

**Configuration**:
```python
CLUSTER_URL = "couchbase://localhost"
USERNAME = "Administrator"
PASSWORD = "password"
JSON_DIRECTORY = "/Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/local_sqlite_json"
SCOPE_NAME = "spider2"
USE_SEPARATE_COLLECTIONS = True
RAM_QUOTA_MB = 600
```

**Usage**:
```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite/utils
python batch_import_to_couchbase.py
```

**Couchbase Structure Created**:
```
AdventureWorks (bucket)
└── spider2 (scope)
    ├── salesperson (collection)
    │   ├── salesperson_274
    │   ├── salesperson_275
    │   └── ... (17 documents)
    ├── product (collection)
    │   ├── product_1
    │   ├── product_2
    │   └── ... (504 documents)
    └── customer (collection)
        └── ... (847 documents)

E_commerce (bucket)
└── spider2 (scope)
    ├── orders (collection)
    ├── customers (collection)
    ├── products (collection)
    └── ... (more collections)
```

**Document Structure**:
Each document includes original data plus metadata:
```json
{
  "order_id": "00143d0f86d6fbd9f9b38ab440000000",
  "customer_id": "00012a2ce6f8dcda20d059ce98491703",
  "order_status": "delivered",
  "order_purchase_timestamp": "2017-10-02 10:56:33",
  "_table": "orders",
  "_source": "E_commerce.json"
}
```

**Document Key Generation**:
- `{table_name}_{id}` - if `id` field exists
- `{table_name}_{businessentityid}` - if `businessentityid` exists
- `{table_name}_{idx}` - uses row index as fallback

**Example Output**:
```
======================================================================
Couchbase Batch JSON Importer
======================================================================

Found 5 JSON files

Files to import:
   1. AdventureWorks.json                      (   45.2 MB)
   2. California_Schools.json                  (   12.3 MB)
   3. E_commerce.json                          (    8.7 MB)
   4. IMDB.json                                (  567.8 MB)
   5. Baseball.json                            (   23.4 MB)

======================================================================
Processing file 1/5: AdventureWorks.json
======================================================================

✓ Bucket 'AdventureWorks' created successfully

Processing table: salesperson (17 rows)
  ✓ Completed: 17 successful, 0 errors

Processing table: product (504 rows)
  ✓ Completed: 504 successful, 0 errors

✓ Successfully imported AdventureWorks.json

======================================================================
BATCH IMPORT SUMMARY
======================================================================
Total files:      5
✓ Successful:     5
✗ Failed:         0
⊘ Skipped:        0
======================================================================
```

**Querying Imported Data**:
```sql
-- Count documents
SELECT COUNT(*) FROM E_commerce.spider2.orders;

-- Query with metadata
SELECT * FROM E_commerce.spider2.orders 
WHERE _table = 'orders' AND order_status = 'delivered';

-- Join across collections
SELECT c.customer_unique_id, o.order_id, o.order_purchase_timestamp
FROM E_commerce.spider2.customers c
JOIN E_commerce.spider2.orders o ON c.customer_id = o.customer_id
WHERE o.order_status = 'delivered'
LIMIT 10;
```

---

## Part II: Testing & SQL Generation

### Overview

This phase uses Couchbase IQ to generate SQL++ queries from natural language questions, executes them against Couchbase, and saves results.

**Two Testing Approaches**:

1. **Quick Test** (`test_couchbase_iq.py`) - Single question for validation
2. **Full Evaluation** (`couchbase_iq_spider2_evaluator.py`) - All 150 local test cases

### Configuration

#### File: `couchbase_config.json`

**Location**: `evaluation_suite/couchbase_config.json`

**Structure**:
```json
{
  "couchbase": {
    "query_endpoint": "http://localhost:8093/query/service",
    "username": "Administrator",
    "password": "password"
  },
  "couchbase_iq": {
    "natural_cred": "your_capella_username:your_capella_password",
    "natural_orgid": "your_capella_org_id"
  }
}
```

**Fields Explained**:
- `query_endpoint` - Couchbase N1QL query service URL
- `username` - Local Couchbase admin username
- `password` - Local Couchbase admin password
- `natural_cred` - Capella credentials (username:password)
- `natural_orgid` - Capella organization ID

### Quick Test: `test_couchbase_iq.py`

**Location**: `evaluation_suite/test_couchbase_iq.py`

**Purpose**: Validates API connectivity and tests single question

**When to Use**:
- ✅ After initial setup (verify everything works)
- ✅ Before full evaluation (quick smoke test)
- ✅ When troubleshooting (isolate issues)
- ✅ Testing configuration changes

**Usage**:
```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite
python test_couchbase_iq.py
```

**5-Step Process**:

1. **Load Configuration** - Reads `couchbase_config.json`
2. **Initialize API** - Creates `CouchbaseIQAPI` instance
3. **Generate SQL** - Converts question to SQL++ using IQ
4. **Execute SQL** - Runs query against Couchbase
5. **Save Results** - Exports to `test_results.csv`

**Test Question**:
```
Could you tell me the number of orders, average payment per order and 
customer lifespan in weeks of the 3 customers with the highest average 
payment per order, where the lifespan is calculated by subtracting the 
earliest purchase date from the latest purchase date in days, dividing 
by seven, and if the result is less than seven days, setting it to 1.0?
```

**This corresponds to instance `local004` from the benchmark**

**Example Output**:
```
================================================================================
Testing Couchbase IQ API
================================================================================

Step 1: Loading configuration...
✓ Configuration loaded
  Query endpoint: http://localhost:8093/query/service
  Username: Administrator
  Natural cred configured: Yes
  Natural orgid configured: Yes

Step 2: Initializing CouchbaseIQ API...
✓ API initialized successfully

Step 3: Testing SQL generation...
  Calling generate_sql()...
✓ SQL generated successfully!

Generated SQL:
--------------------------------------------------------------------------------
SELECT c.customer_unique_id,
       COUNT(o.order_id) AS number_of_orders,
       AVG(p.payment_value) AS average_payment_per_order,
       CASE 
         WHEN DATE_DIFF_STR(MAX(o.order_purchase_timestamp), 
                           MIN(o.order_purchase_timestamp), 'day') / 7 < 1 
         THEN 1.0
         ELSE DATE_DIFF_STR(MAX(o.order_purchase_timestamp), 
                           MIN(o.order_purchase_timestamp), 'day') / 7
       END AS customer_lifespan_weeks
FROM E_commerce.spider2.customers c
JOIN E_commerce.spider2.orders o ON c.customer_id = o.customer_id
JOIN E_commerce.spider2.order_payments p ON o.order_id = p.order_id
GROUP BY c.customer_unique_id
ORDER BY average_payment_per_order DESC
LIMIT 3;
--------------------------------------------------------------------------------

Step 4: Executing generated SQL...
✓ SQL executed successfully!
  Rows returned: 3

Results Preview (first 3 rows):
--------------------------------------------------------------------------------
Row 1:
  customer_unique_id: 0a0a92112bd4c708ca5fde585afaa872
  number_of_orders: 1
  average_payment_per_order: 13664.08
  customer_lifespan_weeks: 1

Row 2:
  customer_unique_id: 763c8b1c9c68a0229c42c9fc6f662b93
  number_of_orders: 1
  average_payment_per_order: 7274.88
  customer_lifespan_weeks: 1

Row 3:
  customer_unique_id: dc4802a71eae9be1dd28f5d788ceb526
  number_of_orders: 1
  average_payment_per_order: 6929.31
  customer_lifespan_weeks: 1
--------------------------------------------------------------------------------

Step 5: Saving results to CSV...
✓ Results saved to: test_results.csv
  Full path: /path/to/evaluation_suite/test_results.csv

================================================================================
✓ TEST PASSED - CouchbaseIQ API is working correctly!
================================================================================
```

**Output File** (`test_results.csv`):
```csv
average_payment_per_order,customer_lifespan_weeks,customer_unique_id,number_of_orders
13664.08,1,0a0a92112bd4c708ca5fde585afaa872,1
7274.88,1,763c8b1c9c68a0229c42c9fc6f662b93,1
6929.31,1,dc4802a71eae9be1dd28f5d788ceb526,1
```

**Key Functions**:

- `load_config()` - Loads couchbase_config.json
- `CouchbaseIQAPI()` - Initializes API client
- `generate_sql(keyspaces, question, evidence)` - Calls IQ API
- `exec_sql(sql_query, ...)` - Executes against Couchbase
- `json_to_csv(results, csv_file)` - Exports results

**Troubleshooting**:

| Error | Cause | Solution |
|-------|-------|----------|
| Credentials not configured | Placeholders in config | Edit couchbase_config.json |
| API initialization failed | Couchbase not running | Start Couchbase Server |
| No SQL generated | Auth issues | Verify Capella credentials |
| SQL execution failed | Missing data | Check bucket/collections exist |

### Full Evaluation: `couchbase_iq_spider2_evaluator.py`

**Location**: `evaluation_suite/couchbase_iq_spider2_evaluator.py`

**Purpose**: Processes all 150 local test cases from Spider2-lite benchmark

**When to Use**:
- ✅ Production evaluation runs
- ✅ Benchmark testing
- ✅ Complete accuracy assessment
- ✅ Batch processing all questions

**Usage**:
```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite
python couchbase_iq_spider2_evaluator.py --mode exec_result
```

**Process**:
1. Loads all instances from `spider2-lite.jsonl`
2. Filters for `local*` instances (SQLite-based)
3. For each instance:
   - Retrieves question and database info
   - Queries Couchbase for relevant keyspaces
   - Calls Couchbase IQ to generate SQL++
   - Executes SQL against Couchbase
   - Saves results to `iq_results/{instance_id}.csv`
4. Reports progress and completion

**Output Directory** (`iq_results/`):
```
iq_results/
├── local002.csv      # Time series prediction
├── local003.csv      # RFM segmentation
├── local004.csv      # Customer analysis
├── local007.csv      # Baseball career spans
├── local008.csv      # Top players
├── local009.csv      # Airline routes
├── local010.csv      # Distance distribution
└── ... (up to 150 CSV files)
```

**Differences from Test Script**:

| Aspect | test_couchbase_iq.py | couchbase_iq_spider2_evaluator.py |
|--------|---------------------|----------------------------------|
| Questions | 1 (hardcoded) | 150 (from JSONL) |
| Purpose | Quick validation | Full benchmark |
| Output | test_results.csv | iq_results/*.csv |
| Runtime | ~10 seconds | ~30-60 minutes |
| Use case | Setup verification | Production evaluation |

---

## Part III: Evaluation & Scoring

### Overview

This phase compares generated results against gold standard outputs to calculate accuracy scores.

**Key Challenge**: Spider2-lite includes 548 total test cases, but only 150 are SQLite-based ("local" instances). We need to focus evaluation on these.

### Cleanup: `cleanup_non_local_results.sh`

**Location**: `evaluation_suite/utils/cleanup_non_local_results.sh`

**Purpose**: Removes non-SQLite gold result files

**Why Needed?**

Spider2-lite gold results include files from multiple database systems:
- `local*.csv` - SQLite (150 files) ✅ **Keep these**
- `sf_bq*.csv` - Snowflake/BigQuery (248 files) ❌ Remove
- `sf_ga*.csv` - Google Analytics ❌ Remove
- `bq*.csv` - BigQuery ❌ Remove

**Reason**: Couchbase IQ tests run against SQLite backends, so only local gold files are relevant for comparison.

**Usage**:
```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite/utils
./cleanup_non_local_results.sh
```

**Interactive Prompt**:
```
Files in /path/to/exec_result:
  Total files: 398
  Local files: 150
  Files to delete: 248

Do you want to delete 248 non-local files? (y/n): y

✓ Deletion complete!
  Remaining files: 150 (all local files)
```

**What It Does**:
```bash
# Finds and deletes all files that DON'T start with "local"
cd gold/exec_result
find . -maxdepth 1 -type f ! -name 'local*.csv' -delete
```

**Before Cleanup**:
```
gold/exec_result/ (398 files, ~15 MB)
├── local002.csv              ← Keep
├── local003_a.csv            ← Keep
├── local004_a.csv            ← Keep
├── sf_bq420.csv              ← Delete (Snowflake)
├── sf_bq421.csv              ← Delete (Snowflake)
├── sf_ga001.csv              ← Delete (GA)
├── sf_local004.csv           ← Delete (SF local, not same as local004)
└── ... (395 more files)
```

**After Cleanup**:
```
gold/exec_result/ (150 files, ~2 MB)
├── local002.csv              ✓
├── local003_a.csv            ✓
├── local003_b.csv            ✓
├── local004_a.csv            ✓
├── local004_b.csv            ✓
└── ... (145 more local files) ✓
```

**Storage Savings**: ~13 MB freed, 248 unnecessary files removed

**Note**: This is a one-time operation. Re-run if gold files are re-downloaded.

### Evaluation: `evaluate.py`

**Location**: `evaluation_suite/evaluate.py`

**Purpose**: Compares test results against gold results and calculates accuracy

**Usage**:
```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite
python evaluate.py --mode exec_result --result_dir iq_results --gold_dir gold
```

**Parameters**:
- `--mode exec_result` - Compare CSV execution results (not SQL)
- `--result_dir iq_results` - Directory with test results
- `--gold_dir gold` - Directory with gold/reference results

**Process**:

1. **Discover Files**
   - Scans `iq_results/` for CSV files
   - Extracts instance IDs (e.g., `local004.csv` → `local004`)
   - Matches against `gold/exec_result/` files

2. **Handle Multiple Variants**
   - Some tests have multiple valid answers
   - Example: `local004_a.csv`, `local004_b.csv`
   - Compares against all variants
   - Takes best score

3. **Compare Data**
   - Loads both CSVs as pandas DataFrames
   - Handles column ordering (uses `condition_cols` from metadata)
   - Compares values with tolerance for floats
   - Ignores order if specified

4. **Score Results**
   - 1 = Perfect match
   - 0 = Mismatch (Result Error, SQL Error, etc.)

5. **Generate Summary**
   - Total score (percentage)
   - Correct examples count
   - Error breakdown

**Matching Logic**:

```python
# Your result: iq_results/local004.csv
# Gold files: gold/exec_result/local004_a.csv, local004_b.csv

# Pattern matching
pattern = r'^local004(_[a-z])?\.csv$'
csv_files = [f for f in os.listdir(gold_dir) if pattern.match(f)]
# Result: ['local004_a.csv', 'local004_b.csv']

# Compare against each variant
scores = []
for gold_file in csv_files:
    score = compare_pandas_table(pred_df, gold_df, condition_cols, ignore_order)
    scores.append(score)

# Take best score
final_score = max(scores)  # 1 if any variant matches
```

**File Naming Convention**:

| Your File | Gold Files | Match Result |
|-----------|-----------|--------------|
| `local004.csv` | `local004_a.csv`, `local004_b.csv` | Matches both ✓ |
| `local004_b.csv` | `local004_a.csv`, `local004_b.csv` | ❌ NO MATCH (wrong naming) |
| `local007.csv` | `local007.csv` | Matches ✓ |
| `local003.csv` | `local003_a.csv`, `local003_b.csv` | Matches both ✓ |

**Important**: Always name your results with base instance ID (e.g., `local004.csv`), not with variant suffix (e.g., `local004_b.csv`). The `_a`/`_b` suffixes are only for gold files.

**Example Output**:
```
0it [00:00, ?it/s]
Processing: local002... ✓ Score: 1
Processing: local003... ✓ Score: 1
Processing: local004... ✓ Score: 1
Processing: local007... ✗ Score: 0 (Result Error)
Processing: local008... ✓ Score: 1
Processing: local009... ✓ Score: 1
Processing: local010... ✗ Score: 0 (SQL Error)
...
150it [00:05, 28.5it/s]

Final score: 0.847, Correct examples: 127, Total examples: 150

Breakdown:
- Correct (Score=1): 127
- Result Error: 15
- SQL Error: 8
- Total: 150
```

**Output Files**:
- `log.txt` - Detailed evaluation log with all comparisons
- Console output - Summary statistics

**Error Types**:

| Error Type | Meaning | Common Causes |
|------------|---------|---------------|
| Result Error | Data mismatch | Wrong values, missing columns, incorrect logic |
| SQL Error | Query failed | Syntax error, missing tables, execution timeout |
| Python Script Error | Comparison failed | Type mismatch, parsing error |

**Evaluation Metadata**: `gold/spider2lite_eval.jsonl`

This file contains evaluation settings for each instance:

```json
{
  "instance_id": "local004",
  "condition_cols": [1, 2, 3],
  "ignore_order": true,
  "toks": "70"
}
```

- `condition_cols` - Columns to use for sorting/matching
- `ignore_order` - Whether row order matters
- `toks` - Token count (for reference)

---

## Part IV: Utility Tools

### Keyspace Explorer: `get_all_keyspaces.py`

**Location**: `evaluation_suite/utils/get_all_keyspaces.py`

**Purpose**: Queries Couchbase to list all buckets, scopes, and collections

**Why Needed**: Couchbase IQ needs the full keyspace list (bucket.scope.collection) to understand what data is available.

**Usage**:
```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite/utils
python get_all_keyspaces.py
```

**Output Example**:
```
Querying Couchbase at http://localhost:8093/query/service...

Found keyspaces:

AdventureWorks.spider2.salesperson
AdventureWorks.spider2.product
AdventureWorks.spider2.customer
AdventureWorks.spider2.salesorderheader
AdventureWorks.spider2.salesorderdetail

E_commerce.spider2.customers
E_commerce.spider2.orders
E_commerce.spider2.products
E_commerce.spider2.order_items
E_commerce.spider2.order_payments
E_commerce.spider2.order_reviews

Baseball.spider2.batting
Baseball.spider2.pitching
Baseball.spider2.player
Baseball.spider2.team

Total keyspaces: 47
```

**Use Case**: 
- Feed this list to Couchbase IQ when generating SQL
- Helps IQ understand available tables/collections
- Useful for debugging missing data

---

---

### Mixed Type Column Analyzer & Cleaner: `find_mixed_type_columns.py`

**Location**: `evaluation_suite/utils/find_mixed_type_columns.py`

**Purpose**: Analyzes JSON files to identify columns with mixed types AND cleans them by replacing empty strings with NULL in numeric columns

**Why Needed**: 

This tool helps identify and fix SQLite type affinity issues in exported JSON files. SQLite's dynamic typing allows empty strings in numeric columns, but Couchbase SQL++ requires strict type consistency. This tool can both diagnose and fix these issues directly in JSON files.

**The Problem It Identifies**:

SQLite's dynamic typing allows columns to contain multiple types:
- A `NUMERIC` column can contain both integers (`42`) and empty strings (`""`)
- A `REAL` column can contain both floats (`3.14`) and integers (`5`)
- This type mixing is preserved when exporting to JSON

**What It Detects**:

The tool identifies columns with:
- `empty_string, int` - Empty strings mixed with integers (most common issue)
- `empty_string, float` - Empty strings mixed with floats
- `float, int` - Integers and floats in the same column
- `empty_string, float, int` - All three types present
- `empty_string, float, int, string` - Even more complex mixing

**Usage**:

```bash
cd /Users/soham.sarkar/Documents/evaluations/Spider2/spider2-lite/evaluation_suite/utils

# Analyze only (default mode)
python3 find_mixed_type_columns.py

# Clean JSON files (replace empty strings with NULL in numeric columns)
python3 find_mixed_type_columns.py --clean

# Dry run - see what would be cleaned without making changes
python3 find_mixed_type_columns.py --clean --dry-run
```

**Output Example**:

```
Analyzing JSON files for mixed type columns...
================================================================================

Processing: AdventureWorks.json
  Table: salesperson
    - salesquota: empty_string, int
    - territoryid: empty_string, int
  Table: product
    - productmodelid: empty_string, int
    - productsubcategoryid: empty_string, int
    - weight: empty_string, float
  Table: salesorderheader
    - creditcardid: empty_string, int
    - currencyrateid: empty_string, int
    - salespersonid: empty_string, int

Processing: Baseball.json
  Table: player
    - birth_year: empty_string, int
    - death_year: empty_string, int
    - height: empty_string, int
    - weight: empty_string, int
  Table: pitching
    - era: empty_string, float, int
    - baopp: empty_string, float, int
  ...

================================================================================
SUMMARY
================================================================================
Total files analyzed: 30
Files with mixed type columns: 7

Type combinations found:
  empty_string, int: 77 columns
  float, int, null: 40 columns
  float, int: 11 columns
  empty_string, float, int: 3 columns
  empty_string, float: 1 columns
  empty_string, float, int, string: 1 columns

Detailed report saved to: /path/to/mixed_type_columns_report.txt
```

**Report File**:

The tool generates a detailed text report (`mixed_type_columns_report.txt`) listing:
- All files with mixed type columns
- All tables with issues
- All affected columns
- The specific type combinations found

**Use Cases**:

| Scenario | When to Use |
|----------|-------------|
| **Post-export diagnosis** | After exporting to JSON, identify which files have type issues |
| **Post-export cleaning** | Clean JSON files by replacing empty strings with NULL in numeric columns |
| **Data quality audit** | Periodically check JSON files for type consistency |
| **Troubleshooting** | When SQL++ queries fail with type errors, identify problematic columns |
| **Before Couchbase import** | Clean JSON files to ensure they're ready for import (should have minimal mixed types) |

**Integration with Data Pipeline**:

```
SQLite Databases
    ↓
export_sqlite_to_json.py
    ↓
JSON Files (may have empty strings in numeric columns)
    ↓
[find_mixed_type_columns.py --clean] ← CLEAN: Replace empty strings with NULL
    ↓
JSON Files (cleaned, ready for import)
    ↓
batch_import_to_couchbase.py
    ↓
Couchbase (SQL++ compatible)
```

**Optional: Analysis Before Cleaning**:

```
export_sqlite_to_json.py
    ↓
[find_mixed_type_columns.py] ← ANALYZE: Identify type issues
    ↓
[find_mixed_type_columns.py --clean] ← CLEAN: Fix issues
    ↓
batch_import_to_couchbase.py
```

**Key Functions**:

- `get_value_type(value)` - Classifies value types (int, float, string, empty_string, null)
- `analyze_column_types(table_data)` - Analyzes all columns in a table
- `is_mixed_type(types)` - Determines if a column has problematic type mixing
- `is_numeric_column(column_types)` - Checks if a column is numeric (contains int or float)
- `clean_json_file(json_path, dry_run)` - Cleans a JSON file by replacing empty strings with NULL
- `analyze_json_file(json_path)` - Processes a single JSON file for analysis
- `main()` - Batch processes all JSON files (analysis or cleaning mode)

**Understanding the Results**:

**Critical Issues** (require cleaning):
- `empty_string, int` - 77 columns found
- `empty_string, float` - 1 column found
- `empty_string, float, int` - 3 columns found
- `empty_string, float, int, string` - 1 column found

**Minor Issues** (usually acceptable):
- `float, int` - 11 columns (integers and floats can coexist)
- `float, int, null` - 40 columns (null is acceptable)

**Example Workflow**:

```bash
# 1. Export SQLite to JSON
cd evaluation_suite/utils
python3 export_sqlite_to_json.py

# 2. Analyze JSON files to see what needs cleaning
python3 find_mixed_type_columns.py

# 3. Review the report
cat ../mixed_type_columns_report.txt

# 4. Clean JSON files (replace empty strings with NULL in numeric columns)
python3 find_mixed_type_columns.py --clean

# 5. Verify cleaning worked
python3 find_mixed_type_columns.py
# Should show fewer or no empty_string, int combinations
```

**Cleaning Output Example**:

```
Cleaning JSON files (replacing empty strings with NULL in numeric columns)...
================================================================================

Processing: Baseball.json
  ✓ Replaced 214567 empty strings with NULL in 16 columns

Processing: AdventureWorks.json
  ✓ Replaced 7 empty strings with NULL in 3 columns

================================================================================
CLEANING SUMMARY
================================================================================
Total files processed: 30
Files modified: 7
Total values replaced: 214574
Total columns cleaned: 19
Total tables processed: 15

✓ Cleaning complete! Backup files saved with .backup extension
```

**Real-World Findings**:

Analysis of 30 JSON files revealed:
- **7 files** with mixed type columns
- **77 columns** with `empty_string, int` (the most common issue)
- **Baseball.json** had the most issues (60+ columns)
- **AdventureWorks.json** had 7 problematic columns
- Most files (23/30) had no type mixing issues

**When to Run**:

| Stage | Purpose | Command |
|-------|---------|---------|
| **After export** | Analyze JSON files for type issues | `python3 find_mixed_type_columns.py` |
| **After export** | Clean JSON files (replace empty strings with NULL) | `python3 find_mixed_type_columns.py --clean` |
| **Before import** | Ensure JSON files are ready for Couchbase | `python3 find_mixed_type_columns.py --clean` |
| **After troubleshooting** | Diagnose type-related query failures | `python3 find_mixed_type_columns.py` |
| **Regular maintenance** | Periodic data quality checks | `python3 find_mixed_type_columns.py` |

**Cleaning Features**:

- ✅ **Automatic backup** - Creates `.backup` files before modifying JSON files
- ✅ **Dry-run mode** - Preview changes without modifying files (`--dry-run`)
- ✅ **Statistics** - Reports how many values were replaced per file
- ✅ **Safe operation** - Only replaces empty strings in numeric columns (columns that contain int or float values)
- ✅ **Preserves structure** - Maintains JSON formatting and structure

**Note**: The tool can both **analyze** (default) and **clean** (with `--clean` flag) JSON files. Cleaning replaces empty strings with NULL in numeric columns, making JSON files compatible with Couchbase SQL++ strict type checking.

---

## Complete Workflow Guide

### Prerequisites

Before starting, ensure you have:

1. ✅ **Couchbase Server** installed and running
   - Download from: https://www.couchbase.com/downloads
   - Access UI at: http://localhost:8091
   - Default credentials: Administrator/password

2. ✅ **Python 3.8+** with required packages
   ```bash
   pip install couchbase requests pandas tqdm
   ```

3. ✅ **Capella Account** with Couchbase IQ access
   - Sign up at: https://cloud.couchbase.com/
   - Get organization ID and credentials
   - Enable IQ service

4. ✅ **Spider2-lite Repository** cloned
   ```bash
   git clone <spider2-lite-repo>
   cd Spider2/spider2-lite/
   ```

### Complete End-to-End Workflow

#### PHASE 1: One-Time Setup (Data Migration)

**Step 1.0: Export SQLite to JSON**

Export SQLite databases to JSON format:

```bash
cd evaluation_suite/utils
python export_sqlite_to_json.py
```

**Expected Output**:
```
Found 5 SQLite files to process
Processing: E_commerce.sqlite
  Found 11 tables
  ✓ Saved to: E_commerce.json
...
✓ All files processed!
```

**Verify**: Check `local_sqlite_json/` for JSON files

---

**Step 1.1: Clean JSON Files (Required for Couchbase Migration)**

**⚠️ IMPORTANT**: This step is **required** before importing to Couchbase. SQLite's dynamic typing allows empty strings in numeric columns, which are exported to JSON. Couchbase SQL++ enforces strict type checking. Without cleaning, SQL++ queries will fail with type errors.

**Optional: Analyze First**

If you want to see which JSON files have type mixing issues before cleaning:

```bash
cd evaluation_suite/utils

# Analyze JSON files for mixed types
python3 find_mixed_type_columns.py

# Review the report to see which files need cleaning
cat ../mixed_type_columns_report.txt
```

**Clean JSON files by replacing empty strings with NULL in numeric columns:**

```bash
cd evaluation_suite/utils

# Dry run - see what would be cleaned without making changes
python3 find_mixed_type_columns.py --clean --dry-run

# Actually clean the JSON files
python3 find_mixed_type_columns.py --clean
```

**Why required**: 
- SQLite exports empty strings as `""` in JSON files for numeric columns
- Couchbase SQL++ requires consistent numeric types (strict typing)
- Without cleaning: SQL++ queries fail with `TypeError: cannot perform arithmetic on string`
- With cleaning: Empty strings are replaced with NULL, ensuring SQL++ compatibility

**Expected Output**:
```
Cleaning JSON files (replacing empty strings with NULL in numeric columns)...
================================================================================

Processing: Baseball.json
  ✓ Replaced 214567 empty strings with NULL in 16 columns

Processing: AdventureWorks.json
  ✓ Replaced 7 empty strings with NULL in 3 columns

================================================================================
CLEANING SUMMARY
================================================================================
Total files processed: 30
Files modified: 7
Total values replaced: 214574
Total columns cleaned: 19

✓ Cleaning complete! Backup files saved with .backup extension
```

**Verify**: Re-run analysis to confirm cleaning worked:

```bash
# Should show fewer or no empty_string, int combinations
python3 find_mixed_type_columns.py
```

---

**Step 1.2: Import JSON to Couchbase**

```bash
cd evaluation_suite/utils
python batch_import_to_couchbase.py
```

**Expected Output**:
```
Found 5 JSON files
Processing file 1/5: E_commerce.json
✓ Bucket 'E_commerce' created
✓ Successfully imported E_commerce.json
...
Total files: 5
✓ Successful: 5
```

**Verify**: 
- Visit http://localhost:8091
- Check buckets exist (E_commerce, Baseball, etc.)
- Check collections are populated

---

**Step 1.3: Configure Credentials**

Create `evaluation_suite/couchbase_config.json`:

```json
{
  "couchbase": {
    "query_endpoint": "http://localhost:8093/query/service",
    "username": "Administrator",
    "password": "password"
  },
  "couchbase_iq": {
    "natural_cred": "your_capella_username:your_capella_password",
    "natural_orgid": "your_capella_org_id"
  }
}
```

**Get Capella credentials**:
1. Log in to Capella
2. Go to Organization Settings
3. Copy Organization ID
4. Create API credentials (username:password)

---

**Step 1.4: Clean Gold Results**

```bash
cd evaluation_suite/utils
./cleanup_non_local_results.sh
```

**Prompt**:
```
Files to delete: 248
Do you want to delete? (y/n): y
✓ Deletion complete!
```

**Verify**: `gold/exec_result/` should have only 150 local*.csv files

---

#### PHASE 2: Testing (Repeatable)

**Step 2.1: Quick Test (Recommended)**

```bash
cd evaluation_suite
python test_couchbase_iq.py
```

**Expected Output**:
```
================================================================================
✓ TEST PASSED - CouchbaseIQ API is working correctly!
================================================================================
```

**If fails**: 
- Check Couchbase is running
- Verify credentials in config
- Check data exists in E_commerce bucket

---

**Step 2.2: (Optional) Quick Evaluation of Test Result**

```bash
# Rename test result to match instance ID
mv test_results.csv iq_results/local004.csv

# Evaluate just this one file
python evaluate.py --mode exec_result --result_dir iq_results --gold_dir gold
```

**Expected Output**:
```
Final score: 1.0, Correct examples: 1, Total examples: 1
```

If score is 1.0, your setup is perfect! Proceed to full evaluation.

---

**Step 2.3: Full Evaluation (All 150 Test Cases)**

```bash
cd evaluation_suite
python couchbase_iq_spider2_evaluator.py --mode exec_result
```

**Expected Output**:
```
Processing local instances...
local002: ✓ SQL generated and executed
local003: ✓ SQL generated and executed
local004: ✓ SQL generated and executed
...
150/150 instances completed
Results saved to iq_results/
```

**Runtime**: 30-60 minutes depending on IQ API speed

**Verify**: `iq_results/` should have ~150 CSV files

---

#### PHASE 3: Evaluation & Scoring

**Step 3.1: Run Evaluation**

```bash
cd evaluation_suite
python evaluate.py --mode exec_result --result_dir iq_results --gold_dir gold
```

**Expected Output**:
```
150it [00:05, 28.5it/s]

Final score: 0.847, Correct examples: 127, Total examples: 150
```

---

**Step 3.2: Analyze Results**

Check `log.txt` for detailed breakdown:

```bash
cat log.txt | grep "Score: 0"
```

**Find errors**:
```
local007: Score: 0 (Result Error)
local010: Score: 0 (SQL Error)
local015: Score: 0 (Result Error)
```

**Debug specific instance**:
```bash
# Compare your result with gold
diff iq_results/local007.csv gold/exec_result/local007.csv

# Check generated SQL (if logged)
grep -A 20 "local007" log.txt
```

---

**Step 3.3: Iterate and Improve**

1. Fix issues (SQL generation, data mapping, etc.)
2. Re-run affected test cases
3. Re-evaluate
4. Repeat until score improves

---

### Workflow Summary

```
┌─────────────────────────────────────────────────────────────────────┐
│                   COMPLETE WORKFLOW CHECKLIST                        │
└─────────────────────────────────────────────────────────────────────┘

SETUP (One-time, ~30 minutes)
 ☐ Install Couchbase Server
 ☐ Install Python packages
 ☐ Create Capella account
 ☐ Export SQLite → JSON (5 minutes)
 ☐ Clean JSON files (2 minutes) ⚠️ Required for SQL++ compatibility
 ☐ Import JSON → Couchbase (10 minutes)
 ☐ Configure couchbase_config.json
 ☐ Clean gold results (1 minute)
 ☐ Run quick test (1 minute)

TESTING (Per evaluation run, ~1 hour)
 ☐ Run full evaluator (30-60 minutes)
 ☐ Verify iq_results/ has 150 files
 
EVALUATION (Per run, ~5 minutes)
 ☐ Run evaluate.py
 ☐ Check final score
 ☐ Analyze log.txt
 ☐ Identify errors
 
ITERATION (As needed)
 ☐ Fix SQL generation issues
 ☐ Re-run affected tests
 ☐ Re-evaluate
 ☐ Track score improvements
```

---

## Quick Reference

### Common Commands

```bash
# ========================================
# SETUP COMMANDS
# ========================================

# Export SQLite to JSON
cd evaluation_suite/utils
python export_sqlite_to_json.py

# Clean JSON files (replace empty strings with NULL in numeric columns)
python3 find_mixed_type_columns.py --clean

# Import to Couchbase
python batch_import_to_couchbase.py

# Clean gold results
./cleanup_non_local_results.sh

# Check keyspaces
python get_all_keyspaces.py

# Analyze mixed type columns in JSON files
python3 find_mixed_type_columns.py

# Clean JSON files (replace empty strings with NULL in numeric columns)
python3 find_mixed_type_columns.py --clean

# ========================================
# TESTING COMMANDS
# ========================================

# Quick test (single question)
cd evaluation_suite
python test_couchbase_iq.py

# Full evaluation (all 150 tests)
python couchbase_iq_spider2_evaluator.py --mode exec_result

# ========================================
# EVALUATION COMMANDS
# ========================================

# Evaluate results
python evaluate.py --mode exec_result --result_dir iq_results --gold_dir gold

# View detailed log
cat log.txt

# Find errors
grep "Score: 0" log.txt

# ========================================
# DEBUGGING COMMANDS
# ========================================

# Check Couchbase is running
curl http://localhost:8091

# Query N1QL service
curl -u Administrator:password \
  -d "statement=SELECT COUNT(*) FROM E_commerce.spider2.orders" \
  http://localhost:8093/query/service

# Compare specific result
diff iq_results/local004.csv gold/exec_result/local004_a.csv

# List generated results
ls -lh iq_results/

# Check gold files
ls -lh gold/exec_result/ | wc -l  # Should be 150 after cleanup
```

### File Locations Cheat Sheet

```
PROJECT ROOT: Spider2/spider2-lite/

DATA:
├── local_sqlite/               ← Source SQLite DBs
├── local_sqlite_json/          ← Generated JSON files
├── spider2-lite.jsonl          ← Benchmark questions

EVALUATION SUITE:
└── evaluation_suite/
    ├── couchbase_config.json           ← Credentials (CREATE THIS)
    ├── test_couchbase_iq.py            ← Quick test script
    ├── couchbase_iq_spider2_evaluator.py ← Full evaluator
    ├── evaluate.py                      ← Scoring script
    │
    ├── gold/                           ← Reference results
    │   ├── exec_result/                ← 150 local*.csv files
    │   ├── sql/                        ← Reference SQL queries
    │   └── spider2lite_eval.jsonl      ← Evaluation metadata
    │
    ├── iq_results/                     ← Generated results (CREATE THIS)
    │   └── local*.csv                  ← 150 test results
    │
    └── utils/                          ← Utility scripts
        ├── export_sqlite_to_json.py
        ├── batch_import_to_couchbase.py
        ├── cleanup_non_local_results.sh
        ├── get_all_keyspaces.py
        ├── find_mixed_type_columns.py  ← Analyze & clean JSON for mixed types
        └── ARCHITECTURE.md (this file)
```

### Troubleshooting Guide

| Issue | Check | Solution |
|-------|-------|----------|
| Test script fails | Couchbase running? | `service couchbase-server status` |
| No SQL generated | Credentials configured? | Edit couchbase_config.json |
| SQL execution fails | Data imported? | Check buckets in UI (port 8091) |
| Division by zero | Files in iq_results? | Run evaluator first |
| Low accuracy score | Check log.txt | Analyze Result Errors |
| Import fails | Enough RAM? | Reduce RAM_QUOTA_MB in script |
| Bucket exists error | Re-running? | Normal, script uses IF NOT EXISTS |

### Performance Tips

**For faster imports**:
- Increase RAM_QUOTA_MB if you have memory
- Process fewer databases initially
- Use SSD for Couchbase data directory

**For faster evaluation**:
- Cache keyspaces list (don't query every time)
- Parallelize SQL generation (if IQ supports)
- Save intermediate results

**For better accuracy**:
- Use external knowledge when available
- Provide complete keyspace context
- Review and fix failed queries manually

### API Limits

**Couchbase IQ**:
- Rate limits may apply (check Capella docs)
- Consider delays between API calls
- Cache generated SQL when possible

**N1QL Query Service**:
- Default timeout: 75 seconds
- Adjust with `timeout` parameter
- Monitor query performance

---

## Architecture Summary

This evaluation suite provides a complete end-to-end pipeline for testing Couchbase IQ against the Spider2-lite benchmark:

### Components

**7 Tools Across 3 Phases**:

| Phase | Tool | Input | Output | Runtime |
|-------|------|-------|--------|---------|
| 1. Migration | export_sqlite_to_json.py | SQLite DBs | JSON files | 5 min |
| 1. Migration | batch_import_to_couchbase.py | JSON files | Couchbase buckets | 10 min |
| 2. Testing | test_couchbase_iq.py | 1 question | 1 CSV | 10 sec |
| 2. Testing | couchbase_iq_spider2_evaluator.py | 150 questions | 150 CSVs | 60 min |
| 3. Evaluation | cleanup_non_local_results.sh | 398 files | 150 files | 1 min |
| 3. Evaluation | evaluate.py | Results + Gold | Score | 5 min |
| 4. Utility | get_all_keyspaces.py | Couchbase | Keyspace list | 1 sec |
| 4. Utility | find_mixed_type_columns.py | JSON files | Analysis/Cleaned JSON | 1-5 min |

### Key Features

- ✅ **Complete pipeline** - From SQLite to evaluation score
- ✅ **Production-ready** - Error handling, logging, progress tracking
- ✅ **Flexible** - Test single questions or full benchmark
- ✅ **Focused** - Only SQLite-compatible test cases (150/548)
- ✅ **Reproducible** - Clear workflow, version control friendly
- ✅ **Debuggable** - Detailed logs, comparison tools
- ✅ **Extensible** - Easy to modify for custom tests

### Success Metrics

**Good First Run**:
- Score > 0.70 (70% accuracy)
- < 20 SQL errors
- Most errors are "Result Error" (wrong logic, not failures)

**Excellent Results**:
- Score > 0.85 (85% accuracy)
- < 10 SQL errors
- High-quality SQL generation with correct business logic

**Perfect Score**:
- Score = 1.0 (100% accuracy)
- 0 errors
- All 150 test cases pass

---

## Appendix

### Supported JSON Formats

The import script (`import_to_couchbase.py`) supports three JSON formats:

**Format 1: Table Dictionary with Row Arrays** (PRIMARY - from export script)

This is the default format produced by `export_sqlite_to_json.py`. Each table name maps directly to an array of row objects:

```json
{
  "salesperson": [
    {
      "businessentityid": 274,
      "territoryid": null,
      "salesquota": null,
      "bonus": 0,
      "commissionpct": 0.0,
      "salesytd": 559697.5639
    },
    {
      "businessentityid": 275,
      "territoryid": 2,
      "salesquota": 300000,
      "bonus": 4100
    }
  ],
  "product": [
    {"productid": 1, "name": "Bike", "price": 499.99},
    {"productid": 2, "name": "Helmet", "price": 29.99}
  ],
  "customer": [...]
}
```

**Import behavior**: 
- Each top-level key becomes a **collection** (e.g., `salesperson`, `product`)
- Each array element becomes a **document** in that collection
- Collections are created under the specified **scope** (default: `spider2`)

**Format 2: Extended with Metadata** (Legacy)

```json
{
  "orders": {
    "row_count": 100,
    "data": [
      {"order_id": 1, "status": "delivered"},
      {"order_id": 2, "status": "pending"}
    ]
  }
}
```

**Import behavior**: Extracts the `data` array for import.

**Format 3: Simple Array** (Flat documents)

```json
[
  {"id": 1, "name": "Document 1"},
  {"id": 2, "name": "Document 2"}
]
```

**Import behavior**: All documents go into the `_default` collection.

### SQL++ Query Examples

**Basic Query**:
```sql
SELECT * FROM E_commerce.spider2.orders 
WHERE order_status = 'delivered'
LIMIT 10;
```

**Join Query**:
```sql
SELECT c.customer_unique_id, COUNT(o.order_id) as order_count
FROM E_commerce.spider2.customers c
JOIN E_commerce.spider2.orders o ON c.customer_id = o.customer_id
GROUP BY c.customer_unique_id;
```

**Aggregation**:
```sql
SELECT AVG(payment_value) as avg_payment
FROM E_commerce.spider2.order_payments;
```

### Configuration Examples

**Minimal Config**:
```json
{
  "couchbase": {
    "query_endpoint": "http://localhost:8093/query/service",
    "username": "Administrator",
    "password": "password"
  },
  "couchbase_iq": {
    "natural_cred": "user:pass",
    "natural_orgid": "org-id"
  }
}
```

**Production Config** (with custom settings):
```json
{
  "couchbase": {
    "query_endpoint": "http://production-server:8093/query/service",
    "username": "admin",
    "password": "secure-password",
    "timeout": 120
  },
  "couchbase_iq": {
    "natural_cred": "api-user:api-key",
    "natural_orgid": "prod-org-id",
    "api_endpoint": "https://api.cloud.couchbase.com/iq"
  }
}
```

---

## Document Version

- **Version**: 2.1
- **Last Updated**: December 2024
- **Author**: Spider2-Lite Evaluation Team
- **Changes in 2.1**: 
  - Clarified JSON format: `{table_name: [rows]}` is the primary format
  - Updated import process to explicitly create scope before importing
  - Added detailed JSON format documentation in Appendix
- **Purpose**: Complete architecture documentation for Spider2-lite evaluation with Couchbase IQ

---

**End of Architecture Document**
